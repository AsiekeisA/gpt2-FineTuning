The GPT-2 model has been fine-tuned with questions and answers sourced from stackoverflow. 
Folder "gpt-train'n'run" contains training and testing code. 
Meanwhile, folder "stackOverflow-dataPrepare" includes code for processing data used in fine-tuning the model.
